{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Detected stocks for evaluation: ['NVDA', 'MSFT', 'TSLA', 'AAPL', 'GOOGL', 'AMZN', 'ORCL', 'IBM', 'META', 'NFLX']\n",
      "\n",
      "üìä Evaluating models for NVDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in Gradient Boosting prediction for NVDA: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for NVDA: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for NVDA: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for NVDA ---\n",
      "‚úÖ RMSE: 1.2027\n",
      "‚úÖ MAE: 1.1272\n",
      "‚úÖ MAPE: 7.85%\n",
      "‚úÖ R¬≤ Score: -7.2221\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for NVDA ---\n",
      "‚úÖ RMSE: 7.1491\n",
      "‚úÖ MAE: 7.1368\n",
      "‚úÖ MAPE: 49.29%\n",
      "‚úÖ R¬≤ Score: -289.4991\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluating models for MSFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in Gradient Boosting prediction for MSFT: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for MSFT: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for MSFT: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for MSFT ---\n",
      "‚úÖ RMSE: 13.5270\n",
      "‚úÖ MAE: 13.3349\n",
      "‚úÖ MAPE: 5.61%\n",
      "‚úÖ R¬≤ Score: -34.4512\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for MSFT ---\n",
      "‚úÖ RMSE: 65.3165\n",
      "‚úÖ MAE: 65.2769\n",
      "‚úÖ MAPE: 27.41%\n",
      "‚úÖ R¬≤ Score: -825.5545\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluating models for TSLA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in Gradient Boosting prediction for TSLA: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for TSLA: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for TSLA: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for TSLA ---\n",
      "‚úÖ RMSE: 83.7842\n",
      "‚úÖ MAE: 83.5749\n",
      "‚úÖ MAPE: 71.27%\n",
      "‚úÖ R¬≤ Score: -199.4037\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for TSLA ---\n",
      "‚úÖ RMSE: 188.0725\n",
      "‚úÖ MAE: 187.9794\n",
      "‚úÖ MAPE: 159.99%\n",
      "‚úÖ R¬≤ Score: -1008.7933\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluating models for AAPL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in Gradient Boosting prediction for AAPL: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for AAPL: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for AAPL: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for AAPL ---\n",
      "‚úÖ RMSE: 3.1970\n",
      "‚úÖ MAE: 2.9250\n",
      "‚úÖ MAPE: 2.24%\n",
      "‚úÖ R¬≤ Score: -1.8362\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for AAPL ---\n",
      "‚úÖ RMSE: 25.8028\n",
      "‚úÖ MAE: 25.7329\n",
      "‚úÖ MAPE: 19.90%\n",
      "‚úÖ R¬≤ Score: -183.7458\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluating models for GOOGL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in Gradient Boosting prediction for GOOGL: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for GOOGL: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for GOOGL: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for GOOGL ---\n",
      "‚úÖ RMSE: 30.2885\n",
      "‚úÖ MAE: 30.2688\n",
      "‚úÖ MAPE: 34.47%\n",
      "‚úÖ R¬≤ Score: -767.5496\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for GOOGL ---\n",
      "‚úÖ RMSE: 51.0311\n",
      "‚úÖ MAE: 51.0194\n",
      "‚úÖ MAPE: 58.09%\n",
      "‚úÖ R¬≤ Score: -2180.6533\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluating models for AMZN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in Gradient Boosting prediction for AMZN: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for AMZN: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for AMZN: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for AMZN ---\n",
      "‚úÖ RMSE: 79.2451\n",
      "‚úÖ MAE: 79.2366\n",
      "‚úÖ MAPE: 94.75%\n",
      "‚úÖ R¬≤ Score: -4696.5656\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for AMZN ---\n",
      "‚úÖ RMSE: 96.9258\n",
      "‚úÖ MAE: 96.9189\n",
      "‚úÖ MAPE: 115.89%\n",
      "‚úÖ R¬≤ Score: -7026.6082\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluating models for ORCL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in Gradient Boosting prediction for ORCL: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for ORCL: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for ORCL: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for ORCL ---\n",
      "‚úÖ RMSE: 2.2034\n",
      "‚úÖ MAE: 2.1501\n",
      "‚úÖ MAPE: 2.65%\n",
      "‚úÖ R¬≤ Score: -19.9016\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for ORCL ---\n",
      "‚úÖ RMSE: 11.4165\n",
      "‚úÖ MAE: 11.4064\n",
      "‚úÖ MAPE: 14.08%\n",
      "‚úÖ R¬≤ Score: -560.1241\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluating models for IBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in Gradient Boosting prediction for IBM: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for IBM: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for IBM: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for IBM ---\n",
      "‚úÖ RMSE: 3.7389\n",
      "‚úÖ MAE: 3.6523\n",
      "‚úÖ MAPE: 2.58%\n",
      "‚úÖ R¬≤ Score: -20.8515\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for IBM ---\n",
      "‚úÖ RMSE: 1.8558\n",
      "‚úÖ MAE: 1.6746\n",
      "‚úÖ MAPE: 1.18%\n",
      "‚úÖ R¬≤ Score: -4.3833\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluating models for META...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error in Gradient Boosting prediction for META: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for META: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for META: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for META ---\n",
      "‚úÖ RMSE: 209.3922\n",
      "‚úÖ MAE: 209.3839\n",
      "‚úÖ MAPE: 177.17%\n",
      "‚úÖ R¬≤ Score: -12712.3876\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for META ---\n",
      "‚úÖ RMSE: 265.8602\n",
      "‚úÖ MAE: 265.8537\n",
      "‚úÖ MAPE: 224.95%\n",
      "‚úÖ R¬≤ Score: -20493.9533\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluating models for NFLX...\n",
      "‚ùå Error in Gradient Boosting prediction for NFLX: X has 36 features, but GradientBoostingRegressor is expecting 35 features as input.\n",
      "‚ùå Error in XGBoost prediction for NFLX: Feature shape mismatch, expected: 35, got 36\n",
      "‚ùå Error in LightGBM prediction for NFLX: Number of features of the model must match the input. Model n_features_ is 35 and input n_features is 36\n",
      "\n",
      "üìä --- ARIMA Evaluation for NFLX ---\n",
      "‚úÖ RMSE: 213.2329\n",
      "‚úÖ MAE: 213.1189\n",
      "‚úÖ MAPE: 74.00%\n",
      "‚úÖ R¬≤ Score: -934.4632\n",
      "----------------------------------------\n",
      "\n",
      "üìä --- SARIMA Evaluation for NFLX ---\n",
      "‚úÖ RMSE: 283.3465\n",
      "‚úÖ MAE: 283.2607\n",
      "‚úÖ MAPE: 98.33%\n",
      "‚úÖ R¬≤ Score: -1650.7875\n",
      "----------------------------------------\n",
      "\n",
      "üìä Model Performance Comparison:\n",
      "    stock   model        rmse         mae        mape            r2\n",
      "0    NVDA   ARIMA    1.202745    1.127234    7.854472     -7.222119\n",
      "1    NVDA  SARIMA    7.149146    7.136831   49.288477   -289.499079\n",
      "2    MSFT   ARIMA   13.527026   13.334878    5.607597    -34.451185\n",
      "3    MSFT  SARIMA   65.316471   65.276947   27.414748   -825.554502\n",
      "4    TSLA   ARIMA   83.784168   83.574868   71.273151   -199.403716\n",
      "5    TSLA  SARIMA  188.072511  187.979364  159.986576  -1008.793327\n",
      "6    AAPL   ARIMA    3.197040    2.925044    2.244961     -1.836205\n",
      "7    AAPL  SARIMA   25.802781   25.732853   19.897933   -183.745793\n",
      "8   GOOGL   ARIMA   30.288500   30.268788   34.470528   -767.549595\n",
      "9   GOOGL  SARIMA   51.031071   51.019374   58.090915  -2180.653304\n",
      "10   AMZN   ARIMA   79.245070   79.236635   94.752308  -4696.565635\n",
      "11   AMZN  SARIMA   96.925845   96.918949  115.892792  -7026.608185\n",
      "12   ORCL   ARIMA    2.203408    2.150053    2.648979    -19.901617\n",
      "13   ORCL  SARIMA   11.416541   11.406364   14.075542   -560.124098\n",
      "14    IBM   ARIMA    3.738869    3.652315    2.583355    -20.851549\n",
      "15    IBM  SARIMA    1.855766    1.674555    1.182709     -4.383296\n",
      "16   META   ARIMA  209.392172  209.383937  177.170291 -12712.387590\n",
      "17   META  SARIMA  265.860162  265.853676  224.945516 -20493.953276\n",
      "18   NFLX   ARIMA  213.232871  213.118869   73.999686   -934.463151\n",
      "19   NFLX  SARIMA  283.346528  283.260745   98.334993  -1650.787463\n",
      "\n",
      "üèÜ **Best Model Per Stock:**\n",
      "    stock   model        rmse         mae        mape            r2\n",
      "6    AAPL   ARIMA    3.197040    2.925044    2.244961     -1.836205\n",
      "10   AMZN   ARIMA   79.245070   79.236635   94.752308  -4696.565635\n",
      "8   GOOGL   ARIMA   30.288500   30.268788   34.470528   -767.549595\n",
      "15    IBM  SARIMA    1.855766    1.674555    1.182709     -4.383296\n",
      "16   META   ARIMA  209.392172  209.383937  177.170291 -12712.387590\n",
      "2    MSFT   ARIMA   13.527026   13.334878    5.607597    -34.451185\n",
      "18   NFLX   ARIMA  213.232871  213.118869   73.999686   -934.463151\n",
      "0    NVDA   ARIMA    1.202745    1.127234    7.854472     -7.222119\n",
      "12   ORCL   ARIMA    2.203408    2.150053    2.648979    -19.901617\n",
      "4    TSLA   ARIMA   83.784168   83.574868   71.273151   -199.403716\n",
      "\n",
      "‚úÖ Results saved to 'model_comparison.csv' and 'best_models.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ‚úÖ **Paths to Saved Models**\n",
    "MODEL_DIR = \"../models/\"\n",
    "ARIMA_DIR = os.path.join(MODEL_DIR, \"arima_sarima_models\")\n",
    "BOOSTING_DIR = os.path.join(MODEL_DIR, \"boosting_models\")\n",
    "RESULTS_FILE = \"../data/model_comparison.csv\"\n",
    "FEATURE_DIR = \"../data/feature_engineering data\"\n",
    "\n",
    "# ‚úÖ **Function to Load and Validate Data**\n",
    "def load_stock_data(stock):\n",
    "    \"\"\"Loads the feature-engineered dataset for a stock, ensuring 'Date' column exists.\"\"\"\n",
    "    file_path = os.path.join(FEATURE_DIR, f\"{stock}_boosting_features.csv\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ö†Ô∏è Feature file missing for {stock}, skipping...\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(file_path, parse_dates=[\"Date\"])  # Ensure 'Date' is parsed\n",
    "    if \"Date\" not in df.columns:\n",
    "        print(f\"‚ùå ERROR: 'Date' column not found in {file_path}\")\n",
    "        print(f\"üîç Available columns: {df.columns.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "# ‚úÖ **Function to Load & Predict with Models**\n",
    "def load_predictions(stock):\n",
    "    \"\"\"Loads trained models and makes predictions for a stock.\"\"\"\n",
    "    \n",
    "    df = load_stock_data(stock)\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    last_row = df.iloc[-1].drop(\"Date\", errors=\"ignore\")  # Ensure it doesn't crash if 'Date' is absent\n",
    "\n",
    "    # ‚úÖ **Load Saved Models (Skipping if Missing)**\n",
    "    models = {}\n",
    "    model_paths = {\n",
    "        \"ARIMA\": os.path.join(ARIMA_DIR, f\"{stock}_ARIMA.pkl\"),\n",
    "        \"SARIMA\": os.path.join(ARIMA_DIR, f\"{stock}_SARIMA.pkl\"),\n",
    "        \"Gradient Boosting\": os.path.join(BOOSTING_DIR, f\"{stock}_GradientBoosting.pkl\"),\n",
    "        \"XGBoost\": os.path.join(BOOSTING_DIR, f\"{stock}_XGBoost.pkl\"),\n",
    "        \"LightGBM\": os.path.join(BOOSTING_DIR, f\"{stock}_LightGBM.pkl\"),\n",
    "    }\n",
    "\n",
    "    for model_name, path in model_paths.items():\n",
    "        if os.path.exists(path):\n",
    "            models[model_name] = joblib.load(path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {model_name} model for {stock} not found, skipping...\")\n",
    "\n",
    "    # ‚úÖ **Make Predictions (Only for Available Models)**\n",
    "    predictions = {}\n",
    "\n",
    "    # ‚úÖ **ARIMA/SARIMA Predictions (Use Date Index)**\n",
    "    for model_name in [\"ARIMA\", \"SARIMA\"]:\n",
    "        if model_name in models:\n",
    "            try:\n",
    "                last_index = df.index[-1]\n",
    "                future_dates = pd.date_range(start=last_index, periods=2, freq=\"D\")[1:]  # Predict next day\n",
    "                pred_series = models[model_name].predict(start=future_dates[0], end=future_dates[0])\n",
    "                predictions[model_name] = pred_series.iloc[0]\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in {model_name} prediction for {stock}: {e}\")\n",
    "\n",
    "    # ‚úÖ **Machine Learning Models Predictions**\n",
    "    for model_name in [\"Gradient Boosting\", \"XGBoost\", \"LightGBM\"]:\n",
    "        if model_name in models:\n",
    "            try:\n",
    "                predictions[model_name] = models[model_name].predict([last_row.values])[0]\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in {model_name} prediction for {stock}: {e}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# ‚úÖ **Run Evaluation for All Stocks**\n",
    "if __name__ == \"__main__\":\n",
    "    stock_files = [f for f in os.listdir(FEATURE_DIR) if f.endswith(\"_boosting_features.csv\")]\n",
    "    stock_symbols = [f.split(\"_\")[0] for f in stock_files]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(f\"\\n‚úÖ Detected stocks for evaluation: {stock_symbols}\")\n",
    "\n",
    "    for stock in stock_symbols:\n",
    "        print(f\"\\nüìä Evaluating models for {stock}...\")\n",
    "\n",
    "        predictions = load_predictions(stock)\n",
    "        if predictions is None:\n",
    "            continue  # Skip stock if missing data\n",
    "\n",
    "        # Get actual last known stock price\n",
    "        df = load_stock_data(stock)\n",
    "        y_true = np.array(df[f\"{stock}_Close\"].iloc[-5:])  # Use last 5 days of actual prices\n",
    "\n",
    "        # ‚úÖ **Evaluate Each Model**\n",
    "        for model_name, y_pred in predictions.items():\n",
    "            results.append(evaluate_model(y_true, np.full_like(y_true, y_pred), model_name, stock))\n",
    "\n",
    "    # ‚úÖ **Convert Results to DataFrame**\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # ‚úÖ **Find the Best Model Per Stock**\n",
    "    best_models = results_df.loc[results_df.groupby(\"stock\")[\"rmse\"].idxmin()]\n",
    "\n",
    "    # ‚úÖ **Display & Save Results**\n",
    "    print(\"\\nüìä Model Performance Comparison:\")\n",
    "    print(results_df)\n",
    "\n",
    "    print(\"\\nüèÜ **Best Model Per Stock:**\")\n",
    "    print(best_models)\n",
    "\n",
    "    results_df.to_csv(RESULTS_FILE, index=False)\n",
    "    best_models.to_csv(\"../data/best_models.csv\", index=False)\n",
    "\n",
    "    print(\"\\n‚úÖ Results saved to 'model_comparison.csv' and 'best_models.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating ML models for IBM...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- IBM_Adj Close\nFeature names seen at fit time, yet now missing:\n- IBM_Close\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_file):\n\u001b[1;32m     57\u001b[0m     model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(model_file)\n\u001b[0;32m---> 58\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     59\u001b[0m     result \u001b[38;5;241m=\u001b[39m evaluate_model(y_test, y_pred, stock, model_type)\n\u001b[1;32m     60\u001b[0m     ml_results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:2125\u001b[0m, in \u001b[0;36mGradientBoostingRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict regression target for X.\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \n\u001b[1;32m   2113\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2123\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2125\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   2126\u001b[0m         X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2127\u001b[0m     )\n\u001b[1;32m   2128\u001b[0m     \u001b[38;5;66;03m# In regression we can directly return the raw value from the trees.\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- IBM_Adj Close\nFeature names seen at fit time, yet now missing:\n- IBM_Close\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ‚úÖ **Paths**\n",
    "MODEL_DIR = \"../models/boosting_models/\"  # Path where ML models are stored\n",
    "FEATURE_DIR = \"../data/feature_engineering data/\"  # Feature dataset path\n",
    "BEST_MODEL_FILE = \"../data/best_models.csv\"  # ARIMA & SARIMA best models\n",
    "COMPARISON_FILE = \"../data/model_comparison.csv\"  # ARIMA & SARIMA comparison\n",
    "FINAL_OUTPUT = \"../data/full_model_comparison.csv\"  # Merged comparison file\n",
    "\n",
    "# ‚úÖ **Function to Evaluate Model Performance**\n",
    "def evaluate_model(y_true, y_pred, stock, model_name):\n",
    "    \"\"\"Calculate RMSE, MAE, MAPE, and R¬≤ Score\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {\"stock\": stock, \"model\": model_name, \"rmse\": rmse, \"mae\": mae, \"mape\": mape, \"r2\": r2}\n",
    "\n",
    "# ‚úÖ **Load ARIMA & SARIMA Results**\n",
    "arima_sarima_results = pd.read_csv(COMPARISON_FILE) if os.path.exists(COMPARISON_FILE) else pd.DataFrame()\n",
    "\n",
    "# ‚úÖ **Detect ML Model Files**\n",
    "ml_models = [f for f in os.listdir(MODEL_DIR) if f.endswith(\".pkl\")]\n",
    "stock_symbols = list(set([f.split(\"_\")[0] for f in ml_models]))\n",
    "\n",
    "# ‚úÖ **Store ML Results**\n",
    "ml_results = []\n",
    "\n",
    "# ‚úÖ **Evaluate ML Models for Each Stock**\n",
    "for stock in stock_symbols:\n",
    "    print(f\"\\nüìä Evaluating ML models for {stock}...\")\n",
    "\n",
    "    # ‚úÖ Load Feature Data\n",
    "    feature_file = os.path.join(FEATURE_DIR, f\"{stock}_boosting_features.csv\")\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(f\"‚ùå Feature file missing for {stock}! Skipping...\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(feature_file)\n",
    "    if \"Date\" in df.columns:\n",
    "        df.drop(columns=[\"Date\"], inplace=True)  # Drop Date column if exists\n",
    "\n",
    "    X_test = df.iloc[-50:, :]  # Use last 50 rows as test data\n",
    "    y_test = X_test[f\"{stock}_Close\"].values  # True values\n",
    "    X_test = X_test.drop(columns=[f\"{stock}_Close\"])  # Features only\n",
    "\n",
    "    # ‚úÖ **Load and Evaluate ML Models**\n",
    "    for model_type in [\"GradientBoosting\", \"XGBoost\", \"LightGBM\"]:\n",
    "        model_file = os.path.join(MODEL_DIR, f\"{stock}_{model_type}.pkl\")\n",
    "        \n",
    "        if os.path.exists(model_file):\n",
    "            model = joblib.load(model_file)\n",
    "            y_pred = model.predict(X_test)\n",
    "            result = evaluate_model(y_test, y_pred, stock, model_type)\n",
    "            ml_results.append(result)\n",
    "        else:\n",
    "            print(f\"‚ùå Missing {model_type} model for {stock}\")\n",
    "\n",
    "# ‚úÖ **Convert ML Results to DataFrame**\n",
    "ml_results_df = pd.DataFrame(ml_results)\n",
    "\n",
    "# ‚úÖ **Merge ARIMA/SARIMA and ML Model Comparisons**\n",
    "full_comparison_df = pd.concat([arima_sarima_results, ml_results_df])\n",
    "\n",
    "# ‚úÖ **Find Best Model Per Stock Based on RMSE**\n",
    "best_models_per_stock = full_comparison_df.loc[full_comparison_df.groupby(\"stock\")[\"rmse\"].idxmin()]\n",
    "\n",
    "# ‚úÖ **Save Final Results**\n",
    "full_comparison_df.to_csv(FINAL_OUTPUT, index=False)\n",
    "best_models_per_stock.to_csv(\"../data/best_models_per_stock.csv\", index=False)\n",
    "\n",
    "print(\"\\nüìä **Full Model Performance Comparison Saved!**\")\n",
    "print(\"\\nüèÜ **Best Model Per Stock (Lowest RMSE):**\")\n",
    "print(best_models_per_stock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading ARIMA & SARIMA results...\n",
      "üì• Loading ML Model results...\n",
      "üîÑ Merging ARIMA/SARIMA & ML model results...\n",
      "\n",
      "üèÜ **Final Model Comparison Saved:** ../data/final_model_comparison.csv\n",
      "üèÜ **Best Model per Stock Saved:** ../data/final_best_models.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/s9t73f3s6lj4xm2bpssc4mw00000gn/T/ipykernel_79900/3897553336.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  best_models_df = comparison_df.groupby(\"stock\").apply(lambda x: x.nsmallest(1, \"rmse\")).reset_index(drop=True)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müèÜ **Best Model per Stock Saved:**\u001b[39m\u001b[38;5;124m\"\u001b[39m, BEST_MODEL_OUTPUT)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# ‚úÖ **Display Best Models**\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m  \u001b[38;5;66;03m# Importing tools to display the table\u001b[39;00m\n\u001b[1;32m     35\u001b[0m tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Model Comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mbest_models_df)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ‚úÖ **Define File Paths**\n",
    "DATA_DIR = \"../data\"\n",
    "BEST_MODELS_FILE = os.path.join(DATA_DIR, \"best_models.csv\")  # ARIMA/SARIMA\n",
    "ML_MODELS_FILE = os.path.join(DATA_DIR, \"model_performance_comparison.csv\")  # ML models\n",
    "COMPARISON_OUTPUT = os.path.join(DATA_DIR, \"final_model_comparison.csv\")\n",
    "BEST_MODEL_OUTPUT = os.path.join(DATA_DIR, \"final_best_models.csv\")\n",
    "\n",
    "# ‚úÖ **Step 1: Load ARIMA & SARIMA Results**\n",
    "print(\"üì• Loading ARIMA & SARIMA results...\")\n",
    "arima_sarima_df = pd.read_csv(BEST_MODELS_FILE)\n",
    "\n",
    "# ‚úÖ **Step 2: Load ML Model Results**\n",
    "print(\"üì• Loading ML Model results...\")\n",
    "ml_df = pd.read_csv(ML_MODELS_FILE)\n",
    "\n",
    "# ‚úÖ **Step 3: Merge All Models for Comparison**\n",
    "print(\"üîÑ Merging ARIMA/SARIMA & ML model results...\")\n",
    "comparison_df = pd.concat([arima_sarima_df, ml_df])\n",
    "\n",
    "# ‚úÖ **Step 4: Find the Best Model per Stock**\n",
    "best_models_df = comparison_df.groupby(\"stock\").apply(lambda x: x.nsmallest(1, \"rmse\")).reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ **Step 5: Save & Display Results**\n",
    "comparison_df.to_csv(COMPARISON_OUTPUT, index=False)\n",
    "best_models_df.to_csv(BEST_MODEL_OUTPUT, index=False)\n",
    "\n",
    "print(\"\\nüèÜ **Final Model Comparison Saved:**\", COMPARISON_OUTPUT)\n",
    "print(\"üèÜ **Best Model per Stock Saved:**\", BEST_MODEL_OUTPUT)\n",
    "\n",
    "# ‚úÖ **Step 6: Display Best Models**\n",
    "print(\"\\nüìä **Best Models for Each Stock**\")\n",
    "print(best_models_df)\n",
    "\n",
    "# ‚úÖ **Optional: Display First Few Rows of the Full Comparison**\n",
    "print(\"\\nüìä **Full Model Comparison (First 10 rows)**\")\n",
    "print(comparison_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
